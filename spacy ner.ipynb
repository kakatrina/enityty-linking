{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all required libraries\n",
    "import spacy\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from spacy.util import minibatch, compounding\n",
    "import sys\n",
    "from spacy import displacy\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    }
   ],
   "source": [
    "dframe = pd.read_csv(\"ner.csv\", encoding = \"ISO-8859-1\", error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dframe.drop(['Unnamed: 0', 'lemma', 'next-lemma', 'next-next-lemma', 'next-next-pos',\n",
    "       'next-next-shape', 'next-next-word', 'next-pos', 'next-shape',\n",
    "       'next-word', 'prev-iob', 'prev-lemma', 'prev-pos',\n",
    "       'prev-prev-iob', 'prev-prev-lemma', 'prev-prev-pos', 'prev-prev-shape',\n",
    "       'prev-prev-word', 'prev-shape', 'prev-word',\"pos\",'shape'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('annotate_corpus.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv('annotate_corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1050795 entries, 0 to 1050794\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count    Dtype  \n",
      "---  ------        --------------    -----  \n",
      " 0   sentence_idx  1050794 non-null  float64\n",
      " 1   word          1050794 non-null  object \n",
      " 2   tag           1050794 non-null  object \n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 24.1+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag=list(dataset.tag.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.n_sent = 1\n",
    "        self.dataset = dataset\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w,t in zip(s[\"word\"].values.tolist(),\n",
    "                                                        s[\"tag\"].values.tolist())]\n",
    "        self.grouped = self.dataset.groupby(\"sentence_idx\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35177"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_format(sent):\n",
    "    data=[]\n",
    "    for i in sent:\n",
    "        sentence=[]\n",
    "        start=0\n",
    "        end=0\n",
    "        trigger=0\n",
    "        entitles=[]\n",
    "        for j in i:\n",
    "            sentence.append(j[0])\n",
    "            if trigger==0:\n",
    "                end+=len(j[0])\n",
    "                trigger+=1\n",
    "                entitles.append((start,end,j[1]))\n",
    "            else:\n",
    "                start=end+1\n",
    "                end=start+len(j[0])\n",
    "                entitles.append((start,end,j[1]))\n",
    "\n",
    "        d=' '.join(sentence)\n",
    "        data.append((d,{'entities':entitles}))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=convert_format(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=random.sample(data,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(d,open('spacy_blank_training.pickle','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train with blank model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statring iteration 0\n",
      "{'ner': 8399.06561235348}\n",
      "Statring iteration 1\n",
      "{'ner': 5561.670450416847}\n",
      "Statring iteration 2\n",
      "{'ner': 4627.349648685211}\n",
      "Statring iteration 3\n",
      "{'ner': 4095.175961650999}\n",
      "Statring iteration 4\n",
      "{'ner': 3561.444163593866}\n",
      "Statring iteration 5\n",
      "{'ner': 3232.1402157207604}\n",
      "Statring iteration 6\n",
      "{'ner': 2979.754204158834}\n",
      "Statring iteration 7\n",
      "{'ner': 2885.036320095188}\n",
      "Statring iteration 8\n",
      "{'ner': 2395.0915484483075}\n",
      "Statring iteration 9\n",
      "{'ner': 2260.5362868493785}\n",
      "Statring iteration 10\n",
      "{'ner': 2255.1655550164237}\n",
      "Statring iteration 11\n",
      "{'ner': 1992.5990144615023}\n",
      "Statring iteration 12\n",
      "{'ner': 1854.8667081208223}\n",
      "Statring iteration 13\n",
      "{'ner': 1823.7338688142208}\n",
      "Statring iteration 14\n",
      "{'ner': 1663.5948840292888}\n",
      "Statring iteration 15\n",
      "{'ner': 1474.2891270572677}\n",
      "Statring iteration 16\n",
      "{'ner': 1457.9582199962522}\n",
      "Statring iteration 17\n",
      "{'ner': 1291.1375534874576}\n",
      "Statring iteration 18\n",
      "{'ner': 1336.6704331669966}\n",
      "Statring iteration 19\n",
      "{'ner': 1184.7581889149883}\n"
     ]
    }
   ],
   "source": [
    "def ner_blank(d):\n",
    "\n",
    "    TRAIN_DATA = d\n",
    "    \n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "\n",
    "    # add labels\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "         for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  \n",
    "        optimizer = nlp.begin_training()# only train NER\n",
    "        for itn in range(20):\n",
    "            print(\"Statring iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses={}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                nlp.update(\n",
    "                    [text],  # batch of texts\n",
    "                    [annotations],  # batch of annotations\n",
    "                    drop=0.2,  # dropout - make it harder to memorise data\n",
    "                  # callable to update weights\n",
    "                    losses=losses)\n",
    "            print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "output_dir=Path(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to ner\n"
     ]
    }
   ],
   "source": [
    "if output_dir is not None:\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate \n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "def evaluate(ner_model, examples):\n",
    "\n",
    "    scorer = Scorer()\n",
    "    for sents, ents in examples:\n",
    "        doc_gold = ner_model.make_doc(sents)\n",
    "        gold = GoldParse(doc_gold, entities=ents['entities'])\n",
    "        pred_value = ner_model(sents)\n",
    "        scorer.score(pred_value, gold)\n",
    "    return scorer.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(nlp, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Scorer.score of <spacy.scorer.Scorer object at 0x0000021BE51C0BE0>>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag=tag[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=['a','bb','abd','k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['a', 'bb', 'abd'], dtype='<U3'), array(['k'], dtype='<U3')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.split(l,[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_data(l):\n",
    "    d=[]\n",
    "    retrain=[]\n",
    "    test=[]\n",
    "    for i in l:\n",
    "        train=dataset[dataset['tag']==i]['sentence_idx'].to_list()\n",
    "        train=list(set(train))\n",
    "        train_,test_=np.split(train,[int(len(train)*0.2)])\n",
    "        d.append(train_)\n",
    "        test.append(test_)\n",
    "    retrain=[j for i in d for j in i]\n",
    "    test_=[j for i in test for j in i]\n",
    "    t=[]\n",
    "    for i in retrain:\n",
    "        if i not in t:\n",
    "            t.append(i)\n",
    "    data=dataset[dataset['sentence_idx'].isin(t)]\n",
    "    test_d=dataset[dataset['sentence_idx'].isin(test_)]\n",
    "    \n",
    "    return data,test_d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain,test=retrain_data(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411922"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "904952"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter(retrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_d=convert_format(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11111"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrain_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_ner(data):     \n",
    "    \n",
    "    TRAIN_DATA=data\n",
    "    nlp=spacy.load(\"ner\") \n",
    "\n",
    "    # Getting the ner component\n",
    "    ner=nlp.get_pipe('ner')\n",
    "       \n",
    "    # add labels\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "         for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(20):\n",
    "            print(\"Statring iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses={}\n",
    "            for texts, annotations in TRAIN_DATA:\n",
    "                nlp.update(\n",
    "                    [texts],  # batch of texts\n",
    "                    [annotations],  # batch of annotations\n",
    "                    drop=0.2,  # dropout - make it harder to memorise data\n",
    "                  # callable to update weights\n",
    "                    \n",
    "                    losses=losses)\n",
    "            print(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statring iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yueyu\\anaconda3_new\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"\" And I think they 'll want a one-stop shop in ter...\" with entities \"[(0, 1, 'O'), (2, 5, 'O'), (6, 7, 'O'), (8, 13, 'O...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 35300.300735924546}\n",
      "Statring iteration 1\n",
      "{'ner': 30149.484013022666}\n",
      "Statring iteration 2\n",
      "{'ner': 27850.340095281164}\n",
      "Statring iteration 3\n",
      "{'ner': 26886.93461522671}\n",
      "Statring iteration 4\n",
      "{'ner': 26494.943516155166}\n",
      "Statring iteration 5\n",
      "{'ner': 25682.587797234035}\n",
      "Statring iteration 6\n",
      "{'ner': 25411.61490940239}\n",
      "Statring iteration 7\n",
      "{'ner': 24582.443030167997}\n",
      "Statring iteration 8\n",
      "{'ner': 23921.132220119198}\n",
      "Statring iteration 9\n",
      "{'ner': 24029.681311608656}\n",
      "Statring iteration 10\n",
      "{'ner': 23306.16035063829}\n",
      "Statring iteration 11\n",
      "{'ner': 23077.420118945203}\n",
      "Statring iteration 12\n",
      "{'ner': 22474.846542156836}\n",
      "Statring iteration 13\n",
      "{'ner': 22495.096729559504}\n",
      "Statring iteration 14\n",
      "{'ner': 21846.21764334765}\n",
      "Statring iteration 15\n",
      "{'ner': 21906.165624637404}\n",
      "Statring iteration 16\n",
      "{'ner': 21371.9170684983}\n",
      "Statring iteration 17\n",
      "{'ner': 21291.52914118827}\n",
      "Statring iteration 18\n",
      "{'ner': 21154.28981090123}\n",
      "Statring iteration 19\n",
      "{'ner': 20963.068749496237}\n"
     ]
    }
   ],
   "source": [
    "retrain_ner(retrain_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to ner\n"
     ]
    }
   ],
   "source": [
    "if output_dir is not None:\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter(test)\n",
    "sentences = getter.sentences\n",
    "test=convert_format(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=random.sample(test,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "def evaluate(ner_model, examples):\n",
    "    scorer = Scorer()\n",
    "    for input_, annot in examples:\n",
    "        doc_gold_text = ner_model.make_doc(input_)\n",
    "        gold = GoldParse(doc_gold_text, entities=annot['entities'])\n",
    "        pred_value = ner_model(input_)\n",
    "        scorer.score(pred_value, gold)\n",
    "    return scorer.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain on small sample adding optimizer\n",
    "def retrain_ner(data):     \n",
    "    \n",
    "    TRAIN_DATA=data\n",
    "    nlp=spacy.load(\"ner\") \n",
    "\n",
    "    # Getting the ner component\n",
    "    ner=nlp.get_pipe('ner')\n",
    "       \n",
    "    # add labels\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "         for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "    optimizer = nlp.begin_training()\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(20):\n",
    "            print(\"Statring iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses={}\n",
    "            \n",
    "            for texts, annotations in TRAIN_DATA:\n",
    "                nlp.update(\n",
    "                    [texts],  # batch of texts\n",
    "                    [annotations],  # batch of annotations\n",
    "                    drop=0.2,\n",
    "                    sgd=optimizer,\n",
    "                  # callable to update weights\n",
    "                    \n",
    "                    losses=losses)\n",
    "            print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yueyu\\anaconda3_new\\lib\\site-packages\\spacy\\language.py:635: UserWarning: [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed. The languages with lexeme normalization tables are currently: da, de, el, en, id, lb, pt, ru, sr, ta, th.\n",
      "  proc.begin_training(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statring iteration 0\n",
      "{'ner': 683.099425269623}\n",
      "Statring iteration 1\n",
      "{'ner': 460.15715160107203}\n",
      "Statring iteration 2\n",
      "{'ner': 289.013202121412}\n",
      "Statring iteration 3\n",
      "{'ner': 199.15601010778005}\n",
      "Statring iteration 4\n",
      "{'ner': 132.3267414417312}\n",
      "Statring iteration 5\n",
      "{'ner': 79.32180101299417}\n",
      "Statring iteration 6\n",
      "{'ner': 56.05614689030193}\n",
      "Statring iteration 7\n",
      "{'ner': 58.11145833690745}\n",
      "Statring iteration 8\n",
      "{'ner': 45.37361945653317}\n",
      "Statring iteration 9\n",
      "{'ner': 34.61627961099172}\n",
      "Statring iteration 10\n",
      "{'ner': 41.868522352570245}\n",
      "Statring iteration 11\n",
      "{'ner': 33.65719534584754}\n",
      "Statring iteration 12\n",
      "{'ner': 22.791765024701895}\n",
      "Statring iteration 13\n",
      "{'ner': 23.46970282084221}\n",
      "Statring iteration 14\n",
      "{'ner': 19.08025796348342}\n",
      "Statring iteration 15\n",
      "{'ner': 24.411872420383425}\n",
      "Statring iteration 16\n",
      "{'ner': 18.033285509444053}\n",
      "Statring iteration 17\n",
      "{'ner': 16.417678022951296}\n",
      "Statring iteration 18\n",
      "{'ner': 18.829629871943595}\n",
      "Statring iteration 19\n",
      "{'ner': 17.63433071618141}\n"
     ]
    }
   ],
   "source": [
    "retrain_ner(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to ner\n"
     ]
    }
   ],
   "source": [
    "if output_dir is not None:\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    optimizer = nlp.begin_training()\n",
    "    with nlp.use_params(optimizer.averages):\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=random.sample(test,100)\n",
    "t_re=[]\n",
    "for i in a:\n",
    "    if i not in t:\n",
    "        t_re.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-105-90c5b1d122d0>:8: UserWarning: [W030] Some entities could not be aligned in the text \"In a telephone interview to discuss the issues at ...\" with entities \"[(0, 2, 'O'), (3, 4, 'O'), (5, 14, 'O'), (15, 24, ...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc_gold_text, entities=annot['entities'])\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(nlp,test\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uas': 0.0,\n",
       " 'las': 0.0,\n",
       " 'las_per_type': {'': {'p': 0.0, 'r': 0.0, 'f': 0.0}},\n",
       " 'ents_p': 95.31313820054703,\n",
       " 'ents_r': 95.16303362789607,\n",
       " 'ents_f': 95.23802676933609,\n",
       " 'ents_per_type': {'O': {'p': 98.658400430825,\n",
       "   'r': 98.62251791278179,\n",
       "   'f': 98.64045590855038},\n",
       "  'B-tim': {'p': 85.65512671856882,\n",
       "   'r': 85.51347775756574,\n",
       "   'f': 85.58424362793778},\n",
       "  'B-gpe': {'p': 93.40059790732437,\n",
       "   'r': 88.38048090523338,\n",
       "   'f': 90.82122093023257},\n",
       "  'I-geo': {'p': 76.70120898100173,\n",
       "   'r': 62.93042369278731,\n",
       "   'f': 69.13676344671909},\n",
       "  'B-geo': {'p': 76.20546163849154,\n",
       "   'r': 86.62015549708812,\n",
       "   'f': 81.07973490875384},\n",
       "  'B-nat': {'p': 40.0, 'r': 14.953271028037381, 'f': 21.768707482993193},\n",
       "  'I-gpe': {'p': 54.87804878048781,\n",
       "   'r': 20.930232558139537,\n",
       "   'f': 30.303030303030305},\n",
       "  'I-org': {'p': 65.20393299344501,\n",
       "   'r': 73.14631987471914,\n",
       "   'f': 68.94714886243301},\n",
       "  'B-org': {'p': 68.91235736829327,\n",
       "   'r': 64.0022547914318,\n",
       "   'f': 66.36661211129297},\n",
       "  'I-per': {'p': 80.65947162815334,\n",
       "   'r': 81.4087142475274,\n",
       "   'f': 81.03236106029867},\n",
       "  'B-per': {'p': 79.83322538860104,\n",
       "   'r': 66.48911064661857,\n",
       "   'f': 72.55269837766252},\n",
       "  'B-art': {'p': 18.4, 'r': 5.528846153846153, 'f': 8.502772643253234},\n",
       "  'I-art': {'p': 26.666666666666668,\n",
       "   'r': 8.88888888888889,\n",
       "   'f': 13.333333333333334},\n",
       "  'I-tim': {'p': 74.76284584980237,\n",
       "   'r': 62.508261731658955,\n",
       "   'f': 68.08855291576674},\n",
       "  'I-eve': {'p': 26.923076923076923,\n",
       "   'r': 12.63537906137184,\n",
       "   'f': 17.1990171990172},\n",
       "  'B-eve': {'p': 34.21052631578947,\n",
       "   'r': 31.137724550898206,\n",
       "   'f': 32.60188087774295},\n",
       "  'I-nat': {'p': 28.749999999999996,\n",
       "   'r': 31.944444444444443,\n",
       "   'f': 30.263157894736835}},\n",
       " 'tags_acc': 0.0,\n",
       " 'token_acc': 100.0,\n",
       " 'textcat_score': 0.0,\n",
       " 'textcats_per_cat': {}}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

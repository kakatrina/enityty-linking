{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Json File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "def pilot_cleanup(row_integer, json_to_load):\n",
    "#js= json.load(open(\"pilot_json/corpus/pilot_20180112044101985994_3330.json\"))\n",
    "    js= json.load(open(json_to_load))\n",
    "    title=\"\".join(js[\"rows\"][row_integer][\"title\"])\n",
    "    content=\"\".join(js[\"rows\"][row_integer][\"content\"])\n",
    "    text=title+\".\"+content\n",
    "    content=unidecode(text)\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    # ### actual begining <!-- begin a.*?body area --> ### i did it like this because they randomly mispelled article in <!-- begin araticle body area -->\n",
    "    # ### articles begin <div class=\"aFullContent\">\n",
    "    # ###  articles end <!-- end article body area -->\n",
    "    [tagstring.extract() for tagstring in soup('script')] ### extracts all content not between balance <script> tags\n",
    "    [tagstring.extract() for tagstring in soup('table')] ### extracts all content not between balance <table> tags\n",
    "    [tagstring.extract() for tagstring in soup('iframe')] ### extracts all content not between balance <iframe> tags\n",
    "    [tagstring.extract() for tagstring in soup('blockquote')] ### extracts all content not between balance <iblockquote> tags\n",
    "    [tagstring.extract() for tagstring in soup('ol')] ### extracts all content not between balance <ol> tags\n",
    "    [tagstring.extract() for tagstring in soup('ul')] ### extracts all content not between balance <ul> tags\n",
    "    [tagstring.extract() for tagstring in soup('h1')] ### extracts all content not between balance <h1> tags\n",
    "    [tagstring.extract() for tagstring in soup('h2')] ### extracts all content not between balance <h2> tags\n",
    "    [tagstring.extract() for tagstring in soup('h3')] ### extracts all content not between balance <h3> tags\n",
    "    [tagstring.extract() for tagstring in soup('h4')] ### extracts all content not between balance <h4> tags\n",
    "    [tagstring.extract() for tagstring in soup('h5')] ### extracts all content not between balance <h5> tags\n",
    "    [tagstring.extract() for tagstring in soup('img')] ### extracts all content not between balance <img> tags\n",
    "    filtered_content = soup.find_all('p')\n",
    "    moment = \"\"\n",
    "    for wrapped_paragraph in filtered_content:\n",
    "        moment+= str(wrapped_paragraph.wrap)+\" \"\n",
    "\n",
    "        content = re.sub(r\"<bound.*?>\",\" \",content).strip() ###some article begin with this\n",
    "        content= re.sub(r'/^(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)*\\/?$/', ' ', content,re.MULTILINE)\n",
    "        content = re.sub(r\"<\\!--.*\",\" \",content,re.DOTALL)\n",
    "        content = re.sub(r\"\\.\\s{,2}\\.\\s\",\". \",content,re.DOTALL)\n",
    "        content = re.sub(r\"\\[.*?\\]\",\" \",content,re.DOTALL)\n",
    "        content = re.sub(r\"\\<span.*?</span\\>\",\" \",content,re.DOTALL)\n",
    "        content = content.replace('<div itemprop=\"reviewBody\">',\" \") ###some article begin with this\n",
    "        content = content.replace(\"@\",\" \")\n",
    "        content = content.replace(\"|\",\" \")\n",
    "        content = content.replace(\"/\",\",\")\n",
    "        content = content.replace(\"(\",\",\")\n",
    "        content = content.replace(\")\",\" \")\n",
    "        content = content.replace(\"---\",\". \")\n",
    "        content = content.replace(\"$\",\" \")\n",
    "        content = re.sub(r\"<.*?>\",\" \" ,content,1000,re.DOTALL)\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "for i in glob.glob(r'C:\\Users\\yueyu\\OneDrive\\桌面\\Corpus\\*.json'):\n",
    "    text=[]\n",
    "    text_=pilot_cleanup(0,i)\n",
    "    text.append(text_)\n",
    "    data.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"virginial_clean_pkl.txt\", \"wb\") as fp:   #Pickling\n",
    "     pickle.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import pickle \n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import gensim.downloader\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_vectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "data=pd.read_csv('ner.csv', encoding = \"ISO-8859-1\", error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['Unnamed: 0', 'lemma', 'next-lemma', 'next-next-lemma', 'next-next-pos',\n",
    "       'next-next-shape', 'next-next-word', 'next-pos', 'next-shape',\n",
    "       'next-word', 'prev-iob', 'prev-lemma', 'prev-pos',\n",
    "       'prev-prev-iob', 'prev-prev-lemma', 'prev-prev-pos', 'prev-prev-shape',\n",
    "       'prev-prev-word', 'prev-shape', 'prev-word',\"pos\",'shape'],axis=1)\n",
    "data=data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tag'].value_counts().plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_map(data, token_or_tag):\n",
    "    ''''\n",
    "    generate index for token and tags\n",
    "    for feeding into the network\n",
    "    \n",
    "    ''''\n",
    "    tok2idx = {}\n",
    "    idx2tok = {}\n",
    "    \n",
    "    if token_or_tag == 'token':\n",
    "        vocab = list(set(data['word'].to_list()))\n",
    "    else:\n",
    "        vocab = list(set(data['tag'].to_list()))\n",
    "    \n",
    "    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n",
    "    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}\n",
    "    return tok2idx, idx2tok\n",
    "    \n",
    "def data_group():\n",
    "    data['word_idx'] = data['word'].map(token2idx)\n",
    "    data['tag_idx'] = data['tag'].map(tag2idx)\n",
    "    data_group = data.groupby(['sentence_idx'],as_index=False\n",
    "                               )['word','tag','word_idx','tag_idx'].agg(lambda x: list(x))\n",
    "    return data_group\n",
    "\n",
    "def getitem():\n",
    "    ''''\n",
    "    in case the word is out of vocabulary, i use random vector to fit\n",
    "    '''\n",
    "    new_vec = np.array([random.random() for i in range(300)])\n",
    "    vec.append(new_vec)\n",
    "    return new_vec\n",
    "\n",
    "def matrix(model):\n",
    "    embedding_matrix1=np.zeros((len(token2idx) + 3,300))\n",
    "    for word, v in token2idx.items():\n",
    "        if word in model.wv.vocab:\n",
    "            embedding_matrix1[v]=model[word]\n",
    "        else:\n",
    "            embedding_matrix1[v]=getitem()\n",
    "            \n",
    "    embedding_matrix1[30174]=getitem()      \n",
    "    return embedding_matrix1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_pad_train_test_val(data_group, data):\n",
    "    '''\n",
    "    Pad data to the max length\n",
    "    for token padding, we all pad the last token, \n",
    "    and tag, we pad it to 'O'\n",
    "    '''\n",
    "    n_token = len(list(set(data['word'].to_list())))\n",
    "    n_tag = len(list(set(data['tag'].to_list())))\n",
    "    \n",
    "    tokens = data_group['word_idx'].tolist()\n",
    "    maxlen = max([len(s) for s in tokens])\n",
    "    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value=n_token-1)\n",
    "    \n",
    "    tags = data_group['tag_idx'].tolist()\n",
    "    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value=tag2idx[\"O\"])\n",
    "    \n",
    "    n_tags = len(tag2idx)\n",
    "    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n",
    "    \n",
    "    \n",
    "    train_tokens, test_tokens, train_tags, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)\n",
    "\n",
    "    print(\n",
    "        'train_tokens length:', len(train_tokens),\n",
    "        '\\ntest_tokens length:', len(test_tokens),\n",
    "        '\\ntrain_tags:', len(train_tags),\n",
    "        '\\ntest_tags:', len(test_tags)\n",
    "    )\n",
    "    \n",
    "    return train_tokens, test_tokens, train_tags, test_tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tags=pickle.load(open('train_tags.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens=pickle.load(open('train_tokens.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens=pickle.load(open('test_tokens.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional,Dense\n",
    "from crf2 import CRF\n",
    "# from CRF import CRF\n",
    "\n",
    "class MyBiLSTMCRF:\n",
    "    def __init__(self, vocabSize, maxLen,tagSum,vecSize=300,learning_rate=0.01):\n",
    "        self.vocabSize = vocabSize\n",
    "        self.vecSize = vecSize\n",
    "        self.maxLen = maxLen\n",
    "        self.tagSum = tagSum\n",
    "        self.learning_rate=learning_rate\n",
    "        self.buildBiLSTMCRF()\n",
    "\n",
    "    def buildBiLSTMCRF(self):\n",
    "        myModel=Sequential()\n",
    "        myModel.add(Input(shape=(self.maxLen,)))\n",
    "        myModel.add(Embedding(self.vocabSize, self.vecSize,weights=[embedding_weight]))\n",
    "        myModel.add(Bidirectional(tf.keras.layers.LSTM(\n",
    "                    self.tagSum, return_sequences=True), merge_mode='concat'))\n",
    "        myModel.add(Bidirectional(tf.keras.layers.LSTM(\n",
    "                    self.tagSum, return_sequences=True), merge_mode='concat'))\n",
    "        myModel.add(Dense(self.tagSum))\n",
    "        crf=CRF(self.tagSum,name='crf_layer')\n",
    "        myModel.add(crf)\n",
    "        myModel.compile(Adam(learning_rate=self.learning_rate),loss={'crf_layer': crf.get_loss},accuracy='')\n",
    "        self.myBiLSTMCRF=myModel\n",
    "        \n",
    "    def fit(self,X,y,epochs=100,batch_size=256, verbose=1, validation_split=0.2,transParam=None,):\n",
    "        if len(y.shape)==3:\n",
    "            y=np.argmax(y,axis=-1)\n",
    "        history=self.myBiLSTMCRF.fit(X,y,epochs=epochs,batch_size=batch_size, verbose=verbose, validation_split=validation_split)\n",
    "\n",
    "        return history\n",
    "\n",
    "    def predict(self,X):\n",
    "        preYArr=self.myBiLSTMCRF.predict(X)\n",
    "        return preYArr\n",
    "\n",
    "    def save(self):\n",
    "        save=self.myBiLSTMCRF.save('bi-lstm-crf')\n",
    "        return save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel=MyBiLSTMCRF(30175,140, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 140, 300)          9052500   \n",
      "_________________________________________________________________\n",
      "bidirectional_14 (Bidirectio (None, 140, 34)           43248     \n",
      "_________________________________________________________________\n",
      "bidirectional_15 (Bidirectio (None, 140, 34)           7072      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 140, 17)           595       \n",
      "_________________________________________________________________\n",
      "crf_layer (CRF)              (None, 140)               629       \n",
      "=================================================================\n",
      "Total params: 9,104,044\n",
      "Trainable params: 9,104,044\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myModel.myBiLSTMCRF.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tags=pickle.load(open('train_tags.pickle','rb'))\n",
    "train_tokens=pickle.load(open('train_tokens.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "99/99 [==============================] - 92s 930ms/step - loss: 26.9687 - val_loss: 9.2687\n",
      "Epoch 2/100\n",
      "99/99 [==============================] - 112s 1s/step - loss: 6.0339 - val_loss: 4.2955\n",
      "Epoch 3/100\n",
      "99/99 [==============================] - 113s 1s/step - loss: 2.9916 - val_loss: 3.4455\n",
      "Epoch 4/100\n",
      "99/99 [==============================] - 115s 1s/step - loss: 2.1633 - val_loss: 3.1659\n",
      "Epoch 5/100\n",
      "99/99 [==============================] - 114s 1s/step - loss: 1.7045 - val_loss: 3.1330\n",
      "Epoch 6/100\n",
      "99/99 [==============================] - 115s 1s/step - loss: 1.3737 - val_loss: 3.2210\n",
      "Epoch 7/100\n",
      "99/99 [==============================] - 117s 1s/step - loss: 1.1250 - val_loss: 3.3609\n",
      "Epoch 8/100\n",
      "99/99 [==============================] - 119s 1s/step - loss: 0.9138 - val_loss: 3.5401\n",
      "Epoch 9/100\n",
      "99/99 [==============================] - 116s 1s/step - loss: 0.7460 - val_loss: 3.8093\n",
      "Epoch 10/100\n",
      "99/99 [==============================] - 119s 1s/step - loss: 0.5984 - val_loss: 4.0464\n",
      "Epoch 11/100\n",
      "99/99 [==============================] - 119s 1s/step - loss: 0.4881 - val_loss: 4.3926\n",
      "Epoch 12/100\n",
      "99/99 [==============================] - 112s 1s/step - loss: 0.4021 - val_loss: 4.6643\n",
      "Epoch 13/100\n",
      "99/99 [==============================] - 114s 1s/step - loss: 0.3332 - val_loss: 4.8902\n",
      "Epoch 14/100\n",
      "99/99 [==============================] - 113s 1s/step - loss: 0.2795 - val_loss: 5.1935\n",
      "Epoch 15/100\n",
      "99/99 [==============================] - 109s 1s/step - loss: 0.2374 - val_loss: 5.4346\n",
      "Epoch 16/100\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.2061 - val_loss: 5.7406\n",
      "Epoch 17/100\n",
      "99/99 [==============================] - 109s 1s/step - loss: 0.1684 - val_loss: 5.9949\n",
      "Epoch 18/100\n",
      "99/99 [==============================] - 107s 1s/step - loss: 0.1469 - val_loss: 6.3336\n",
      "Epoch 19/100\n",
      "99/99 [==============================] - 114s 1s/step - loss: 0.1319 - val_loss: 6.5362\n",
      "Epoch 20/100\n",
      "99/99 [==============================] - 114s 1s/step - loss: 0.1425 - val_loss: 6.8495\n",
      "Epoch 21/100\n",
      "99/99 [==============================] - 111s 1s/step - loss: 0.1675 - val_loss: 6.5065\n",
      "Epoch 22/100\n",
      "99/99 [==============================] - 111s 1s/step - loss: 0.2184 - val_loss: 6.4907\n",
      "Epoch 23/100\n",
      "99/99 [==============================] - 112s 1s/step - loss: 0.1912 - val_loss: 6.3455\n",
      "Epoch 24/100\n",
      "99/99 [==============================] - 111s 1s/step - loss: 0.1441 - val_loss: 6.4868\n",
      "Epoch 25/100\n",
      "99/99 [==============================] - 109s 1s/step - loss: 0.1145 - val_loss: 6.8161\n",
      "Epoch 26/100\n",
      "99/99 [==============================] - 108s 1s/step - loss: 0.0942 - val_loss: 6.9508\n",
      "Epoch 27/100\n",
      "99/99 [==============================] - 125s 1s/step - loss: 0.0801 - val_loss: 7.2705\n",
      "Epoch 28/100\n",
      "99/99 [==============================] - 118s 1s/step - loss: 0.0685 - val_loss: 7.4912\n",
      "Epoch 29/100\n",
      "99/99 [==============================] - 118s 1s/step - loss: 0.0648 - val_loss: 7.7149\n",
      "Epoch 30/100\n",
      "99/99 [==============================] - 118s 1s/step - loss: 0.0632 - val_loss: 7.7731\n",
      "Epoch 31/100\n",
      "99/99 [==============================] - 118s 1s/step - loss: 0.0718 - val_loss: 7.7854\n",
      "Epoch 32/100\n",
      "99/99 [==============================] - 116s 1s/step - loss: 0.0766 - val_loss: 8.0712\n",
      "Epoch 33/100\n",
      "99/99 [==============================] - 108s 1s/step - loss: 0.1177 - val_loss: 7.9649\n",
      "Epoch 34/100\n",
      "99/99 [==============================] - 108s 1s/step - loss: 0.2210 - val_loss: 7.4885\n",
      "Epoch 35/100\n",
      "99/99 [==============================] - 108s 1s/step - loss: 0.2527 - val_loss: 6.7897\n",
      "Epoch 36/100\n",
      "99/99 [==============================] - 108s 1s/step - loss: 0.1959 - val_loss: 6.6778\n",
      "Epoch 37/100\n",
      "99/99 [==============================] - 124s 1s/step - loss: 0.1332 - val_loss: 6.8420\n",
      "Epoch 38/100\n",
      "99/99 [==============================] - 111s 1s/step - loss: 0.0854 - val_loss: 7.2565\n",
      "Epoch 39/100\n",
      "99/99 [==============================] - 116s 1s/step - loss: 0.0545 - val_loss: 7.4873\n",
      "Epoch 40/100\n",
      "99/99 [==============================] - 116s 1s/step - loss: 0.0349 - val_loss: 7.8910\n",
      "Epoch 41/100\n",
      "99/99 [==============================] - 117s 1s/step - loss: 0.0221 - val_loss: 8.3620\n",
      "Epoch 42/100\n",
      "99/99 [==============================] - 117s 1s/step - loss: 0.0229 - val_loss: 8.5107\n",
      "Epoch 43/100\n",
      "99/99 [==============================] - 109s 1s/step - loss: 0.0221 - val_loss: 8.8288\n",
      "Epoch 44/100\n",
      "99/99 [==============================] - 109s 1s/step - loss: 0.0191 - val_loss: 9.0732\n",
      "Epoch 45/100\n",
      "99/99 [==============================] - 119s 1s/step - loss: 0.0186 - val_loss: 9.1152\n",
      "Epoch 46/100\n",
      "99/99 [==============================] - 117s 1s/step - loss: 0.0168 - val_loss: 9.3992\n",
      "Epoch 47/100\n",
      "99/99 [==============================] - 116s 1s/step - loss: 0.0176 - val_loss: 9.4400\n",
      "Epoch 48/100\n",
      "99/99 [==============================] - 114s 1s/step - loss: 0.0236 - val_loss: 9.5021\n",
      "Epoch 49/100\n",
      "99/99 [==============================] - 112s 1s/step - loss: 0.0601 - val_loss: 9.4836\n",
      "Epoch 50/100\n",
      "99/99 [==============================] - 113s 1s/step - loss: 0.3665 - val_loss: 6.6370\n",
      "Epoch 51/100\n",
      "99/99 [==============================] - 110s 1s/step - loss: 0.4176 - val_loss: 5.7936\n",
      "Epoch 52/100\n",
      "99/99 [==============================] - 113s 1s/step - loss: 0.2721 - val_loss: 5.9193\n",
      "Epoch 53/100\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.1590 - val_loss: 6.6653\n",
      "Epoch 54/100\n",
      "99/99 [==============================] - 127s 1s/step - loss: 0.0989 - val_loss: 7.1377\n",
      "Epoch 55/100\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.0653 - val_loss: 7.4475\n",
      "Epoch 56/100\n",
      "99/99 [==============================] - 120s 1s/step - loss: 0.0438 - val_loss: 7.9935\n",
      "Epoch 57/100\n",
      "99/99 [==============================] - 120s 1s/step - loss: 0.0294 - val_loss: 8.3686\n",
      "Epoch 58/100\n",
      "99/99 [==============================] - 123s 1s/step - loss: 0.0222 - val_loss: 8.7368\n",
      "Epoch 59/100\n",
      "99/99 [==============================] - 125s 1s/step - loss: 0.0203 - val_loss: 9.1524\n",
      "Epoch 60/100\n",
      "99/99 [==============================] - 130s 1s/step - loss: 0.0150 - val_loss: 9.4945\n",
      "Epoch 61/100\n",
      "99/99 [==============================] - 125s 1s/step - loss: 0.0213 - val_loss: 9.5938\n",
      "Epoch 62/100\n",
      "99/99 [==============================] - 123s 1s/step - loss: 0.0393 - val_loss: 9.5764\n",
      "Epoch 63/100\n",
      "99/99 [==============================] - 120s 1s/step - loss: 0.1164 - val_loss: 9.0721\n",
      "Epoch 64/100\n",
      "99/99 [==============================] - 120s 1s/step - loss: 0.2812 - val_loss: 7.0181\n",
      "Epoch 65/100\n",
      "99/99 [==============================] - 114s 1s/step - loss: 0.2818 - val_loss: 6.3866\n",
      "Epoch 66/100\n",
      "99/99 [==============================] - 112s 1s/step - loss: 0.2321 - val_loss: 6.4173\n",
      "Epoch 67/100\n",
      "99/99 [==============================] - 114s 1s/step - loss: 0.1460 - val_loss: 6.9956\n",
      "Epoch 68/100\n",
      "99/99 [==============================] - 109s 1s/step - loss: 0.1152 - val_loss: 7.2825\n",
      "Epoch 69/100\n",
      "99/99 [==============================] - 113s 1s/step - loss: 0.0755 - val_loss: 7.8646\n",
      "Epoch 70/100\n",
      "99/99 [==============================] - 109s 1s/step - loss: 0.0530 - val_loss: 8.2051\n",
      "Epoch 71/100\n",
      "99/99 [==============================] - 114s 1s/step - loss: 0.0418 - val_loss: 8.5651\n",
      "Epoch 72/100\n",
      "99/99 [==============================] - 112s 1s/step - loss: 0.0416 - val_loss: 8.6947\n",
      "Epoch 73/100\n",
      "99/99 [==============================] - 108s 1s/step - loss: 0.0400 - val_loss: 9.1076\n",
      "Epoch 74/100\n",
      "99/99 [==============================] - 111s 1s/step - loss: 0.0392 - val_loss: 9.2159\n",
      "Epoch 75/100\n",
      "99/99 [==============================] - 112s 1s/step - loss: 0.0586 - val_loss: 9.5641\n",
      "Epoch 76/100\n",
      "99/99 [==============================] - 113s 1s/step - loss: 0.0885 - val_loss: 9.1097\n",
      "Epoch 77/100\n",
      "99/99 [==============================] - 114s 1s/step - loss: 0.1566 - val_loss: 8.3078\n",
      "Epoch 78/100\n",
      "99/99 [==============================] - 112s 1s/step - loss: 0.2079 - val_loss: 7.2441\n",
      "Epoch 79/100\n",
      "99/99 [==============================] - 111s 1s/step - loss: 0.1994 - val_loss: 7.1315\n",
      "Epoch 80/100\n",
      "99/99 [==============================] - 117s 1s/step - loss: 0.1641 - val_loss: 7.2603\n",
      "Epoch 81/100\n",
      "99/99 [==============================] - 107s 1s/step - loss: 0.1212 - val_loss: 7.5553\n",
      "Epoch 82/100\n",
      "99/99 [==============================] - 106s 1s/step - loss: 0.0823 - val_loss: 7.9566\n",
      "Epoch 83/100\n",
      "99/99 [==============================] - 120s 1s/step - loss: 0.0690 - val_loss: 8.3815\n",
      "Epoch 84/100\n",
      "99/99 [==============================] - 119s 1s/step - loss: 0.0543 - val_loss: 8.6206\n",
      "Epoch 85/100\n",
      "99/99 [==============================] - 118s 1s/step - loss: 0.0473 - val_loss: 8.9416\n",
      "Epoch 86/100\n",
      "99/99 [==============================] - 119s 1s/step - loss: 0.0417 - val_loss: 9.2248\n",
      "Epoch 87/100\n",
      "99/99 [==============================] - 119s 1s/step - loss: 0.0383 - val_loss: 9.4316\n",
      "Epoch 88/100\n",
      "99/99 [==============================] - 117s 1s/step - loss: 0.0376 - val_loss: 9.6393\n",
      "Epoch 89/100\n",
      "99/99 [==============================] - 107s 1s/step - loss: 0.0493 - val_loss: 9.7892\n",
      "Epoch 90/100\n",
      "99/99 [==============================] - 107s 1s/step - loss: 0.0682 - val_loss: 9.7206\n",
      "Epoch 91/100\n",
      "99/99 [==============================] - 107s 1s/step - loss: 0.1312 - val_loss: 9.0199\n",
      "Epoch 92/100\n",
      "99/99 [==============================] - 106s 1s/step - loss: 0.2042 - val_loss: 7.7271\n",
      "Epoch 93/100\n",
      "99/99 [==============================] - 106s 1s/step - loss: 0.2160 - val_loss: 7.3346\n",
      "Epoch 94/100\n",
      "99/99 [==============================] - 107s 1s/step - loss: 0.1960 - val_loss: 7.0617\n",
      "Epoch 95/100\n",
      "99/99 [==============================] - 106s 1s/step - loss: 0.1552 - val_loss: 7.4531\n",
      "Epoch 96/100\n",
      "99/99 [==============================] - 107s 1s/step - loss: 0.1152 - val_loss: 7.7476\n",
      "Epoch 97/100\n",
      "99/99 [==============================] - 106s 1s/step - loss: 0.0991 - val_loss: 8.1408\n",
      "Epoch 98/100\n",
      "99/99 [==============================] - 107s 1s/step - loss: 0.0751 - val_loss: 8.4318\n",
      "Epoch 99/100\n",
      "99/99 [==============================] - 112s 1s/step - loss: 0.0672 - val_loss: 8.7545\n",
      "Epoch 100/100\n",
      "99/99 [==============================] - 119s 1s/step - loss: 0.0597 - val_loss: 9.0730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ef169d7c70>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myModel.fit(train_tokens,np.array(train_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report,precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx=pickle.load(open('tag2idx.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens=pickle.load(open('test_tokens.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred=myModel.predict(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "idx2tag={}\n",
    "for i,v in tag2idx.items():\n",
    "    idx2tag[v]=i\n",
    "    \n",
    "ct=[]\n",
    "for i,v in idx2tag.items():\n",
    "    ct.append(list(to_categorical(i, num_classes=18)))\n",
    "tags_nd=dict(zip(idx2tag.keys(),ct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tags=pickle.load(open('test_tags.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-eve',\n",
       " 1: 'I-per',\n",
       " 2: 'I-org',\n",
       " 3: 'B-geo',\n",
       " 4: 'B-art',\n",
       " 5: 'B-tim',\n",
       " 6: 'I-gpe',\n",
       " 7: 'O',\n",
       " 8: 'I-geo',\n",
       " 9: 'B-org',\n",
       " 10: 'I-art',\n",
       " 11: 'B-per',\n",
       " 12: 'B-nat',\n",
       " 13: 'I-tim',\n",
       " 14: 'B-gpe',\n",
       " 15: 'I-nat',\n",
       " 16: 'I-eve'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truelabel(pred):\n",
    "    out = []\n",
    "    for i in pred:\n",
    "        out_i = []\n",
    "        for p in i:\n",
    "            p_i = np.argmax(p)\n",
    "            out_i.append(idx2tag[p_i])\n",
    "        out.append(out_i)\n",
    "    return out\n",
    "\n",
    "def testlabel(pred):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out_i = []\n",
    "        for p in pred_i:\n",
    "            for k,v in tags_nd.items():\n",
    "                if k==p:\n",
    "                    m=idx2tag[k]\n",
    "                    out_i.append(m)\n",
    "        out.append(out_i)\n",
    "    return out\n",
    "    \n",
    "true_labels = truelabel(test_tags)\n",
    "test_labels = testlabel(test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         art       0.21      0.17      0.19        30\n",
      "         eve       0.29      0.28      0.29        32\n",
      "         geo       0.84      0.83      0.83      3829\n",
      "         gpe       0.93      0.90      0.91      1712\n",
      "         nat       0.10      0.10      0.10        21\n",
      "         org       0.61      0.64      0.63      2004\n",
      "         per       0.72      0.71      0.72      1698\n",
      "         tim       0.84      0.82      0.83      2016\n",
      "\n",
      "   micro avg       0.79      0.78      0.79     11342\n",
      "   macro avg       0.57      0.56      0.56     11342\n",
      "weighted avg       0.79      0.78      0.79     11342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_labels,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
